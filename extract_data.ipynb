{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extract data from iPyrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following takes `*.stats.txt` files (which are output from iPyrad) and produces three files: a `_SNPdist.csv` file, which contains numbers of variable and parsimony-informative sites, a `_sumstats.csv` file, which contains numbers of clusters, consensus reads, and loci in assembly for each sample, and finally a `_coverage.csv` file, which contains stats about coverage/sequencing depth.\n",
    "\n",
    "For each sampling depth, it merges all .csv files into single files, either `samplingdepth_ddrad_sumstats.csv`, `samplingdepth_ddrad_snpdist.csv`, or `samplingdepth_ddrad_coverage.csv`.\n",
    "\n",
    "Finally, it does the exact same process as above but instead of differing sampling depths, it differs clustering threshold values; end files are outputted as `clustthreshold_ddrad_sumstats.csv`, `clustthreshold_ddrad_snpdist.csv`, `clustthreshold_ddrad_coverage.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load relevant modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import itertools\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test that glob is working\n",
    "\n",
    "The following is just a check step to make a list of file names we're going to iterate through using glob.\n",
    "The `stats.txt` files are the ones we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranafilenames = glob.glob('sampling_depth/ranaddrad*outfiles/*stats.txt')\n",
    "print ranafilenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define directory\n",
    "\n",
    "Define the directory for which you want all your files produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#directory='sampling_depth/*outfiles/*stats.txt' # for sampling depths t1, t2, t3, total\n",
    "directory='clust_threshold/*outfiles/*stats.txt' # for clustering threshold iterations 80-95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNP distributions\n",
    "\n",
    "The following pulls info about the number of variable and parsimony-informative (as iPyrad calculates it) sites and produces `_SNPdist.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to grab file names\n",
    "def get_file_list(directory):\n",
    "    return glob.glob(directory)\n",
    "\n",
    "# Function to reference line numbers to pull out specific sections of the stats file\n",
    "def get_line_numbers(filename):\n",
    "    counter=0\n",
    "    infile=open(filename, 'rt')\n",
    "    for line in infile:\n",
    "        counter+=1\n",
    "        if 'var  sum_var' in line:\n",
    "            varline=counter\n",
    "#            print name,varline # not necessary but want to just check\n",
    "#            print line\n",
    "        elif '## Final' in line:\n",
    "            endline=counter\n",
    "#            print name,endline\n",
    "#            print line\n",
    "        else:\n",
    "            continue\n",
    "    infile.close()\n",
    "    return [varline,endline]\n",
    "\n",
    "# Function to cut out the SNP distribution section\n",
    "def slice_data(start,end,filename):\n",
    "    snp_dist=[]\n",
    "    infile=open(filename,'rU')\n",
    "    for lines in itertools.islice(infile, start, end-3):\n",
    "        lines2 = lines.strip().split()\n",
    "#        print lines2\n",
    "        lines2.append(filename) # eventually change to whatever file it's on\n",
    "        snp_dist.append(lines2)\n",
    "    infile.close()\n",
    "    return snp_dist\n",
    "        \n",
    "# Function to convert the output to an actual dataframe format that we can read into R\n",
    "def pd_conversion(filename):\n",
    "    nums = get_line_numbers(filename)\n",
    "    snp_dist = slice_data(nums[0],nums[1],filename)\n",
    "    snpdist_labels=['number','variable','sum_var','pis', 'sum_pis', 'file_name']\n",
    "    df_snpdist = pd.DataFrame.from_records(snp_dist, columns=snpdist_labels)\n",
    "    return df_snpdist  \n",
    "\n",
    "def main():\n",
    "    file_list = get_file_list(directory)\n",
    "    dfs=[]\n",
    "    for filename in file_list:\n",
    "        print filename\n",
    "        pd_df = pd_conversion(filename)\n",
    "        dfs.append(pd_df)\n",
    "        pd_df.to_csv(filename+\"_snpdist.csv\")\n",
    "    return dfs\n",
    "\n",
    "print main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary statistics\n",
    "\n",
    "The following pulls info about the number of clusters, consensus reads, and loci in assembly for each sample and produces `_sumstats.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_list(directory):\n",
    "    return glob.glob(directory)\n",
    "\n",
    "def get_line_numbers(filename):\n",
    "    counter=0\n",
    "    infile=open(filename, 'rt')\n",
    "    for line in infile:\n",
    "        counter+=1\n",
    "        if 'state  reads_raw' in line:\n",
    "            varline=counter\n",
    "#            print name,varline # not necessary but want to just check\n",
    "#        print line\n",
    "        else:\n",
    "            continue\n",
    "    infile.close()\n",
    "    return [varline,varline]\n",
    "\n",
    "def slice_data(start,end,filename):\n",
    "    sum_stats=[]\n",
    "    infile=open(filename,'rU')\n",
    "    for lines in itertools.islice(infile, start, end+12):\n",
    "        lines2 = lines.strip().split()\n",
    "        print lines2\n",
    "        lines2.append(filename) # eventually change to whatever file it's on\n",
    "        sum_stats.append(lines2)\n",
    "    infile.close()\n",
    "    return sum_stats\n",
    "        \n",
    "\n",
    "def pd_conversion(filename):\n",
    "    nums = get_line_numbers(filename)\n",
    "    sum_stats = slice_data(nums[0],nums[1],filename)\n",
    "    sumstats_labels=['sample', 'state', 'reads_raw', 'reads_passed', 'clust_total', 'clust_hidepth','hetero_est','error_est','reads_consens','loci_assembly','file_name']\n",
    "    df_sumstats = pd.DataFrame.from_records(sum_stats, columns=sumstats_labels)\n",
    "    return df_sumstats  \n",
    "\n",
    "def main():\n",
    "    file_list = get_file_list(directory)\n",
    "    dfs=[]\n",
    "    for filename in file_list:\n",
    "        print filename\n",
    "        pd_df = pd_conversion(filename)\n",
    "        dfs.append(pd_df)\n",
    "        pd_df.to_csv(\"./\" +filename+ \"_sumstats.csv\")\n",
    "    return dfs\n",
    "\n",
    "print main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coverage information\n",
    "\n",
    "The following pulls info about the number of loci for which N taxa have data and produces `_coverage.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_list(directory):\n",
    "    return glob.glob(directory)\n",
    "\n",
    "def get_line_numbers(filename):\n",
    "    counter=0\n",
    "    infile=open(filename, 'rt')\n",
    "    for line in infile:\n",
    "        counter+=1\n",
    "        if 'locus_coverage' in line:\n",
    "            varline=counter\n",
    "#            print name,varline # not necessary but want to just check\n",
    "#        print line\n",
    "        elif '## The distribution' in line:\n",
    "            endline=counter\n",
    "            print filename,endline\n",
    "        else:\n",
    "            continue\n",
    "    infile.close()\n",
    "    return [varline,endline]\n",
    "\n",
    "def slice_data(start,end,filename):\n",
    "    coverage=[]\n",
    "    infile=open(filename,'rU')\n",
    "    for lines in itertools.islice(infile, start, end-3):\n",
    "        lines2 = lines.strip().split()\n",
    "        print lines2\n",
    "        lines2.append(filename) # eventually change to whatever file it's on\n",
    "        coverage.append(lines2)\n",
    "    infile.close()\n",
    "    return coverage\n",
    "\n",
    "def pd_conversion(filename):\n",
    "    nums = get_line_numbers(filename)\n",
    "    coverage = slice_data(nums[0],nums[1],filename)\n",
    "    cov_labels=['number','locus_coverage', 'sum_coverage','file_name']\n",
    "    df_cov = pd.DataFrame.from_records(coverage, columns=cov_labels)\n",
    "    return df_cov  \n",
    "\n",
    "def main():\n",
    "    file_list = get_file_list(directory)\n",
    "    dfs=[]\n",
    "    for filename in file_list:\n",
    "        print filename\n",
    "        pd_df = pd_conversion(filename)\n",
    "        dfs.append(pd_df)\n",
    "        pd_df.to_csv(\"./\" +filename+ \"_coverage.csv\")\n",
    "    return dfs\n",
    "\n",
    "print main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge dataframes into a single file\n",
    "\n",
    "All above code will produce separate files for each of the sampling depths (or each of the clustering thresholds). We want these all to be merged into a single file for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling depths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Epipedobates\n",
    "df1 = pd.read_csv(\"./sampling_depth/epiddrad_t1_4_outfiles/epiddrad_t1_stats.txt_sumstats.csv\")\n",
    "df2 = pd.read_csv(\"./sampling_depth/epiddrad_t2_4_outfiles/epiddrad_t2_stats.txt_sumstats.csv\")\n",
    "df3 = pd.read_csv(\"./sampling_depth/epiddrad_t3_4_outfiles/epiddrad_t3_stats.txt_sumstats.csv\")\n",
    "df4 = pd.read_csv(\"./sampling_depth/epiddrad_total_4_outfiles/epiddrad_clust_91_stats.txt_sumstats.csv\")\n",
    "\n",
    "# For Rana\n",
    "df5 = pd.read_csv(\"./sampling_depth/ranaddrad_t1_4_outfiles/ranaddrad_t1_stats.txt_sumstats.csv\")\n",
    "df6 = pd.read_csv(\"./sampling_depth/ranaddrad_t2_4_outfiles/ranaddrad_t2_stats.txt_sumstats.csv\")\n",
    "df7 = pd.read_csv(\"./sampling_depth/ranaddrad_t3_4_outfiles/ranaddrad_t3_stats.txt_sumstats.csv\")\n",
    "df8 = pd.read_csv(\"./sampling_depth/ranaddrad_total_4_outfiles/ranaddrad_clust_91_stats.txt_sumstats.csv\")\n",
    "\n",
    "# Concatenate everything together into single file\n",
    "sumstats_concat = df1.append([df2,df3,df4,df5,df6,df7,df8])\n",
    "sumstats_concat.to_csv(\"samplingdepth_ddrad_sumstats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNP distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Epipedobates\n",
    "df1 = pd.read_csv(\"./sampling_depth/epiddrad_t1_4_outfiles/epiddrad_t1_stats.txt_snpdist.csv\")\n",
    "df2 = pd.read_csv(\"./sampling_depth/epiddrad_t2_4_outfiles/epiddrad_t2_stats.txt_snpdist.csv\")\n",
    "df3 = pd.read_csv(\"./sampling_depth/epiddrad_t3_4_outfiles/epiddrad_t3_stats.txt_snpdist.csv\")\n",
    "df4 = pd.read_csv(\"./sampling_depth/epiddrad_total_4_outfiles/epiddrad_clust_91_stats.txt_snpdist.csv\")\n",
    "\n",
    "# For Rana\n",
    "df5 = pd.read_csv(\"./sampling_depth/ranaddrad_t1_4_outfiles/ranaddrad_t1_stats.txt_snpdist.csv\")\n",
    "df6 = pd.read_csv(\"./sampling_depth/ranaddrad_t2_4_outfiles/ranaddrad_t2_stats.txt_snpdist.csv\")\n",
    "df7 = pd.read_csv(\"./sampling_depth/ranaddrad_t3_4_outfiles/ranaddrad_t3_stats.txt_snpdist.csv\")\n",
    "df8 = pd.read_csv(\"./sampling_depth/ranaddrad_total_4_outfiles/ranaddrad_clust_91_stats.txt_snpdist.csv\")\n",
    "\n",
    "# Concatenate everything together into single file\n",
    "sumstats_concat = df1.append([df2,df3,df4,df5,df6,df7,df8])\n",
    "sumstats_concat.to_csv(\"samplingdepth_ddrad_snpdist.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Epipedobates\n",
    "df1 = pd.read_csv(\"./sampling_depth/epiddrad_t1_4_outfiles/epiddrad_t1_stats.txt_coverage.csv\")\n",
    "df2 = pd.read_csv(\"./sampling_depth/epiddrad_t2_4_outfiles/epiddrad_t2_stats.txt_coverage.csv\")\n",
    "df3 = pd.read_csv(\"./sampling_depth/epiddrad_t3_4_outfiles/epiddrad_t3_stats.txt_coverage.csv\")\n",
    "df4 = pd.read_csv(\"./sampling_depth/epiddrad_total_4_outfiles/epiddrad_clust_91_stats.txt_coverage.csv\")\n",
    "\n",
    "# For Rana\n",
    "df5 = pd.read_csv(\"./sampling_depth/ranaddrad_t1_4_outfiles/ranaddrad_t1_stats.txt_coverage.csv\")\n",
    "df6 = pd.read_csv(\"./sampling_depth/ranaddrad_t2_4_outfiles/ranaddrad_t2_stats.txt_coverage.csv\")\n",
    "df7 = pd.read_csv(\"./sampling_depth/ranaddrad_t3_4_outfiles/ranaddrad_t3_stats.txt_coverage.csv\")\n",
    "df8 = pd.read_csv(\"./sampling_depth/ranaddrad_total_4_outfiles/ranaddrad_clust_91_stats.txt_coverage.csv\")\n",
    "\n",
    "# Concatenate everything together into single file\n",
    "sumstats_concat = df1.append([df2,df3,df4,df5,df6,df7,df8])\n",
    "sumstats_concat.to_csv(\"samplingdepth_ddrad_coverage.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do each of the following for all dfs produced from above and for each Epipedobates and Rana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Epipedobates\n",
    "df1 = pd.read_csv(\"./clust_threshold/epiddrad_clust_80_outfiles/epiddrad_clust_80_stats.txt_sumstats.csv\")\n",
    "# do above for remaining dfs\n",
    "\n",
    "# For Rana\n",
    "df17 = pd.read_csv(\"./clust_threshold/ranaddrad_clust_80_outfiles/ranaddrad_clust_80_stats.txt_sumstats.csv\")\n",
    "# do above for remaining dfs\n",
    "\n",
    "sumstats_concat = df1.append([df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,\n",
    "                             df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32])\n",
    "sumstats_concat.to_csv(\"clustthreshold_ddrad_sumstats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNP distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Epipedobates\n",
    "df1 = pd.read_csv(\"./clust_threshold/epiddrad_clust_80_outfiles/epiddrad_clust_80_stats.txt_snpdist.csv\")\n",
    "# do above for remaining dfs\n",
    "\n",
    "# For Rana\n",
    "df17 = pd.read_csv(\"./clust_threshold/ranaddrad_clust_80_outfiles/ranaddrad_clust_80_stats.txt_snpdist.csv\")\n",
    "# do above for remaining dfs\n",
    "\n",
    "snpdist_concat = df1.append([df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,\n",
    "                             df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32])\n",
    "snpdist_concat.to_csv(\"clustthreshold_ddrad_snpdist.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Epipedobates\n",
    "df1 = pd.read_csv(\"./clust_threshold/epiddrad_clust_80_outfiles/epiddrad_clust_80_stats.txt_coverage.csv\")\n",
    "# do above for remaining dfs\n",
    "\n",
    "# For Rana\n",
    "df17 = pd.read_csv(\"./clust_threshold/ranaddrad_clust_80_outfiles/ranaddrad_clust_80_stats.txt_coverage.csv\")\n",
    "# do above for remaining dfs\n",
    "\n",
    "coverage_concat = df1.append([df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,\n",
    "                             df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30,df31,df32])\n",
    "coverage_concat.to_csv(\"clustthreshold_ddrad_coverage.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
